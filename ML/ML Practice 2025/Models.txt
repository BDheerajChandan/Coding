In machine learning, **supervised** and **unsupervised** learning are two primary categories. Both have different types of models, and each model can be implemented using popular libraries like `scikit-learn` (`sklearn`).

### **Supervised Learning Models:**
These models require labeled data, where the algorithm learns the relationship between input features and the output labels.

#### Common Supervised Models in Scikit-learn:
1. **Linear Models:**
   - **Linear Regression**: Used for regression tasks (predicting continuous values).
     - `from sklearn.linear_model import LinearRegression`
   - **Logistic Regression**: Used for classification tasks (binary or multi-class).
     - `from sklearn.linear_model import LogisticRegression`

2. **Support Vector Machines (SVM):**
   - **SVM for Classification**: Used for classification tasks by finding a hyperplane that best separates classes.
     - `from sklearn.svm import SVC` (Support Vector Classification)
   - **SVM for Regression**: Similar to SVM for classification but used for regression tasks.
     - `from sklearn.svm import SVR` (Support Vector Regression)

3. **Decision Trees:**
   - **Decision Tree Classifier**: Used for classification tasks.
     - `from sklearn.tree import DecisionTreeClassifier`
   - **Decision Tree Regressor**: Used for regression tasks.
     - `from sklearn.tree import DecisionTreeRegressor`

4. **Random Forest:**
   - **Random Forest Classifier**: An ensemble of decision trees used for classification tasks.
     - `from sklearn.ensemble import RandomForestClassifier`
   - **Random Forest Regressor**: An ensemble of decision trees used for regression tasks.
     - `from sklearn.ensemble import RandomForestRegressor`

5. **K-Nearest Neighbors (KNN):**
   - **KNeighborsClassifier**: Used for classification tasks.
     - `from sklearn.neighbors import KNeighborsClassifier`
   - **KNeighborsRegressor**: Used for regression tasks.
     - `from sklearn.neighbors import KNeighborsRegressor`

6. **Naive Bayes:**
   - **GaussianNB**: For classification tasks based on Gaussian distribution of data.
     - `from sklearn.naive_bayes import GaussianNB`
   - **MultinomialNB**: Used for text classification problems.
     - `from sklearn.naive_bayes import MultinomialNB`

7. **Gradient Boosting:**
   - **GradientBoostingClassifier**: For classification tasks using boosting technique.
     - `from sklearn.ensemble import GradientBoostingClassifier`
   - **GradientBoostingRegressor**: For regression tasks using boosting technique.
     - `from sklearn.ensemble import GradientBoostingRegressor`

---

### **Unsupervised Learning Models:**
Unsupervised learning algorithms do not require labeled data. They are used to find hidden structures or patterns in the data.

#### Common Unsupervised Models in Scikit-learn:
1. **Clustering Algorithms:**
   - **K-Means**: A clustering algorithm that partitions data into k clusters.
     - `from sklearn.cluster import KMeans`
   - **DBSCAN**: Density-based spatial clustering.
     - `from sklearn.cluster import DBSCAN`
   - **Agglomerative Clustering**: A hierarchical clustering algorithm.
     - `from sklearn.cluster import AgglomerativeClustering`

2. **Dimensionality Reduction:**
   - **Principal Component Analysis (PCA)**: A technique for reducing the number of features in data while retaining variance.
     - `from sklearn.decomposition import PCA`
   - **t-SNE**: A technique for dimensionality reduction, particularly useful for visualizing high-dimensional data.
     - `from sklearn.manifold import TSNE`
   - **TruncatedSVD**: A dimensionality reduction method based on Singular Value Decomposition.
     - `from sklearn.decomposition import TruncatedSVD`

3. **Gaussian Mixture Models (GMM):**
   - Used for probabilistic clustering, assuming data is generated from a mixture of several Gaussian distributions.
     - `from sklearn.mixture import GaussianMixture`

4. **Isolation Forest:**
   - Used for anomaly detection by isolating observations that deviate from the norm.
     - `from sklearn.ensemble import IsolationForest`

5. **KMeans (also an unsupervised model)**: A clustering method, as mentioned earlier.
   - `from sklearn.cluster import KMeans`

---

### **Packages in Scikit-learn (sklearn)**
Scikit-learn provides a wide range of algorithms and models for both supervised and unsupervised learning. The major packages and modules are:

- **Classification**: `from sklearn.linear_model import LogisticRegression`, `from sklearn.svm import SVC`, `from sklearn.ensemble import RandomForestClassifier`
- **Regression**: `from sklearn.linear_model import LinearRegression`, `from sklearn.tree import DecisionTreeRegressor`
- **Clustering**: `from sklearn.cluster import KMeans`, `from sklearn.cluster import DBSCAN`
- **Dimensionality Reduction**: `from sklearn.decomposition import PCA`, `from sklearn.manifold import TSNE`
- **Model Selection**: `from sklearn.model_selection import train_test_split`, `from sklearn.model_selection import cross_val_score`
- **Metrics**: `from sklearn.metrics import accuracy_score`, `from sklearn.metrics import mean_squared_error`
- **Preprocessing**: `from sklearn.preprocessing import StandardScaler`, `from sklearn.preprocessing import OneHotEncoder`
  
Scikit-learn is built to be easy to use, and these models can be plugged into a pipeline for preprocessing, model fitting, and evaluation.

Let me know if you need further details about any of these models!











#####################################################################


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Select two features (sepal length and sepal width for simplicity)
X = X[:, :2]  # Use only the first two features for visualization

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create and train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=20)
knn.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Plot the decision boundary
h = .02  # Step size for meshgrid
x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Classify each point in the meshgrid
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the points
plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='o', edgecolor='k', label='Train data')
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='^', edgecolor='k', label='Test data')

# Set plot labels
plt.title('KNN Decision Boundary on Iris Dataset')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.legend(loc='best')
plt.show()
